---
title: "Math RL"
date: 2021. 2. 21. 오전 11:04:01
categories:
use_math: true
toc: true
toc_label: "Table of Contents"
toc_icon: "cog"
toc_sticky: true
---

[comment]: <> (포스트 화면 넓게 설정하고 싶을 때 추가, classes: wide)

![Math RL (수학으로 풀어보는 강화학습 원리와 알고리즘)](https://raw.githubusercontent.com/missflash/missflash.github.io/master/_files/math_RL.jpg){: width="30%" height="30%"}


# 1. 강화학습 수학
* 1.1 확률과 랜덤 변수
  * 1.1.1 확률
  * 1.1.2 랜덤 변수
  * 1.1.3 누적분포함수와 확률밀도함수
  * 1.1.4 결합 확률함수
  * 1.1.5 조건부 확률함수
  * 1.1.6 독립 랜덤 변수
  * 1.1.7 랜덤 변수의 함수
  * 1.1.8 베이즈 정리
  * 1.1.9 샘플링
* 1.2 기댓값과 분산
  * 1.2.1 기댓값
  * 1.2.2 분산
  * 1.2.3 조건부 기댓값과 분산
* 1.3 랜덤벡터
  * 1.3.1 정의
  * 1.3.2 기댓값과 공분산 행렬
  * 1.3.3 샘플 평균
* 1.4 가우시안 분포
* 1.5 랜덤 시퀀스
  * 1.5.1 정의
  * 1.5.2 평균함수와 자기 상관함수
  * 1.5.3 마르코프 시퀀스
* 1.6 선형 확률 차분방정식
* 1.7 표기법
* 1.8 중요 샘플링
* 1.9 엔트로피
* 1.10 KL 발산
* 1.11 추정기
  * 1.11.1 최대사후 추정기
  * 1.11.2 최대빈도 추정기
* 1.12 벡터와 행렬의 미분
  * 1.12.1 벡터로 미분
  * 1.12.2 행렬로 미분
* 1.13 촐레스키 분해
* 1.14 경사하강법
  * 1.14.1 배치 경사하강법
  * 1.14.2 확률적 경사하강법
* 1.15 경사하강법의 개선
  * 1.15.1 모멘텀
  * 1.15.2 RMSprop
  * 1.15.3 아담
* 1.16 손실함수의 확률론적 해석
  * 1.16.1 가우시안 오차 분포
  * 1.16.2 베르누이 오차 분포



# 2. 강화학습 개념
* 2.1 강화학습 개요
* 2.2 강화학습 프로세스와 표기법
* 2.3 마르코프 결정 프로세스
  * 2.3.1 정의
  * 2.3.2 가치함수
  * 2.3.3 벨만 방정식
  * 2.3.4 벨만 최적 방정식
* 2.4 강화학습 방법



# 3. 정책 그래디언트
* 3.1 배경
* 3.2 목적함수
* 3.3 정책 그래디언트
* 3.4 REINFORCE 알고리즘



# 4. A2C
* 4.1 배경
* 4.2 그래디언트의 재구성
* 4.3 분산을 감소시키기 위한 방법
* 4.4 A2C 알고리즘
* 4.5 A2C 알고리즘 구현
  * 4.5.1 테스트 환경
  * 4.5.2 코드 개요
  * 4.5.3 액터 클래스
  * 4.5.4 크리틱 클래스
  * 4.5.5 액터-크리틱 에이전트 클래스
  * 4.5.6 학습 결과
  * 4.5.7 전체 코드



# 5. A3C
* 5.1 배경
* 5.2 그래디언트 계산의 문제
  * 5.2.1 샘플의 상관관계
  * 5.2.2 n-스텝 가치 추정
  * 5.2.3 엔트로피 보너스
* 5.3 비동기 액터-크리틱(A3C) 알고리즘
* 5.4 그래디언트 병렬화 방식의 A3C 알고리즘 구현
  * 5.4.1 테스트 환경
  * 5.4.2 코드 개요
  * 5.4.3 액터 클래스
  * 5.4.4 크리틱 클래스
  * 5.4.5 액터-크리틱 에이전트 클래스
  * 5.4.6 학습 결과
  * 5.4.7 전체 코드
* 5.5 데이터 병렬화 방식의 A3C 알고리즘 구현
  * 5.5.1 코드 개요
  * 5.5.2 전체 코드



# 6. PPO
* 6.1 배경
* 6.2 그래디언트의 재구성
* 6.3 정책 업데이트와 성능
* 6.4 PPO 알고리즘
* 6.5 어드밴티지 추정의 일반화 (GAE)
* 6.6 PPO 알고리즘 구현
  * 6.6.1 테스트 환경
  * 6.6.2 코드 개요
  * 6.6.3 액터 클래스
  * 6.6.4 크리틱 클래스
  * 6.6.5 액터-크리틱 에이전트 클래스
  * 6.6.6 학습 결과
  * 6.6.7 전체 코드



# 7. DDPG
* 7.1 배경
* 7.2 그래디언트의 재구성
* 7.3 DDPG 알고리즘
* 7.4 DDPG 알고리즘 구현
  * 7.4.1 테스트 환경
  * 7.4.2 코드 개요
  * 7.4.3 액터 클래스
  * 7.4.4 크리틱 클래스
  * 7.4.5 액터-크리틱 에이전트 클래스
  * 7.4.6 학습 결과
  * 7.4.7 전체 코드



# 8. 모델 기반 강화학습 기초
* 8.1 배경
* 8.2 최적제어
  * 8.2.1 LQR
  * 8.2.2 확률적 LQR
  * 8.2.3 가우시안 LQR
  * 8.2.4 반복적 LQR
* 8.3 모델 학습 방법



# 9. 로컬 모델 기반 강화학습
* 9.1 배경
* 9.2 로컬 모델 피팅 기반 LQR
* 9.3 로컬 모델 피팅
  * 9.3.1 조건부 가우시안 방법
  * 9.3.2 GMM 사전분포를 이용한 로컬 모델 업데이트
* 9.4 로컬 제어 법칙 업데이트
  * 9.4.1 대체 비용함수 계산
  * 9.4.2 KL 발산 계산<br>
  * 9.4.3 $\eta$ 조정<br>
  * 9.4.4 $\epsilon$ 조정<br>
* 9.5 가우시안 LQR을 이용한 강화학습 알고리즘
* 9.6 가우시안 LQR을 이용한 강화학습 알고리즘 구현
  * 9.6.1 테스트 환경
  * 9.6.2 코드 개요
  * 9.6.3 궤적 생성
  * 9.6.4 로컬 모델 피팅
  * 9.6.5 가우시안 LQR
  * 9.6.6 가우시안 혼합 모델
  * 9.6.7 LQR-FLM 에이전트 클래스
  * 9.6.8 학습 결과
  * 9.6.9 전체 코드
* 9.7 GPS로의 발전



# 참고자료
* [https://github.com/pasus/Reinforcement-Learning-Book](https://github.com/pasus/Reinforcement-Learning-Book)
* [https://github.com/Yeachan-Heo/Reinforcement-Learning-Book](https://github.com/Yeachan-Heo/Reinforcement-Learning-Book)
