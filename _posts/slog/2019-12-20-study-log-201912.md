---
title: "Study Log (2019.12)"
date: 2019. 11. 29. 오후 8:32:38
categories:
use_math: true
classes: wide
---

# 2019-12-28
* [Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html)
  * Chapter 7. n-step Bootstrapping
    * 7.1 n-step TD Prediction
      * 1) n-step까지 Discounted Reward 합계 G 계산 : $G \leftarrow \sum\nolimits_{i=\tau+1}^{min(\tau+n,T)} \gamma^{i-\tau-1} R_i$<br>
      * 2) n-step에서의 Value 계산 (n-step 이후의 Reward 함축) : $G \leftarrow G + \gamma^n V(\color{red}{S_{\tau+n}})$<br>
      * 3) V 업데이트 : $V(S_\tau) \leftarrow V(S_\tau) + \alpha [G - V(\color{red}{S_\tau)}]$<br>
      * [RandomWalk.py](https://github.com/JaeDukSeo/reinforcement-learning-an-introduction/blob/master/chapter07/RandomWalk.py)<br>
      ![n-step TD for estimating V](https://raw.githubusercontent.com/missflash/missflash.github.io/master/_files/n-step TD for estimating V.png){: width="70%" height="70%"}
    * 7.2
    * 7.3
    * 7.4
  * Page #150
* [N-step TD Method](https://medium.com/zero-equals-false/n-step-td-method-157d3875b9cb)

---

# 2019-12-27
* [Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html)
  * Chapter 6. Temporal-Difference Learning
    * 6.6 Expected Sarsa
    * 6.7 Maximization Bias and Double Learning
    * 6.8 Games, Afterstates, and Other Special Cases
    * 6.9 Summary
  * Chapter 7. n-step Bootstrapping
    * 7.1 n-step TD Prediction
  * Page #143
* [모두를 위한 머신러닝/딥러닝 강의](http://hunkim.github.io/ml/)
  * [Lecture #33) lec10-4: 레고처럼 넷트웍 모듈을 마음껏 쌓아 보자](https://www.youtube.com/watch?v=YHsbHjTBx9Q&list=PLlMkM4tgfjnLSOjrEJN31gZATbcj_MpUm&index=34)
  * Lecture #34
  * Lecture #35
  * Lecture #36
  * Lecture #37
  * Lecture #38
  * Lecture #39

---

# 2019-12-25
* [Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html)
  * Chapter 6. Temporal-Difference Learning
    * 6.1 TD Prediction
    * 6.2 Advantages of TD Prediction Methods
    * 6.3 Optimality of TD(0)
    * 6.4 Sarsa: On-policy TD Control
    * 6.5 Q-learning: Off-policy TD Control
  * Page #133

---

# 2019-12-24
* [SanghyukChun's Blog](http://sanghyukchun.github.io/)
  * [Machine Learning 스터디 (10) PAC Learning & Statistical Learning Theory](http://sanghyukchun.github.io/66/)
  * [Machine Learning 스터디 (17) Recommendation System (Matrix Completion)](http://sanghyukchun.github.io/73/)
  * [Machine Learning 스터디 (17-1) Recommendation System With Implicit Feedback](http://sanghyukchun.github.io/95/)
  * [Machine Learning 스터디 (20-1) Multi-armed Bandit](http://sanghyukchun.github.io/96/)
* [Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html)
  * Chapter 5. Monte Carlo Methods
    * 5.7 Off-policy Monte Carlo Control
    * 5.8 Discounting-aware Importance Sampling
    * 5.9 Per-decision Importance Sampling
    * 5.10 Summary
  * Page #119

---

# 2019-12-23
* [SanghyukChun's Blog](http://sanghyukchun.github.io/)
  * [Machine Learning 스터디 (18) Neural Network Introduction](http://sanghyukchun.github.io/74/)
  * [Machine Learning 스터디 (19) Deep Learning - RBM, DBN, CNN](http://sanghyukchun.github.io/75/)
  * [Machine learning 스터디 (20) Reinforcement Learning](http://sanghyukchun.github.io/76/)

---

# 2019-12-22
* [SanghyukChun's Blog](http://sanghyukchun.github.io/)
  * [Machine Learning 스터디 (5) Decision Theory](http://sanghyukchun.github.io/61/)
  * [Machine learning 스터디 (6) Information Theory](http://sanghyukchun.github.io/62/)
  * [Machine learning 스터디 (7) Convex Optimization](http://sanghyukchun.github.io/63/)
  * [Machine learning 스터디 (8) Classification Introduction (Decision Tree, Naïve Bayes, KNN)](http://sanghyukchun.github.io/64/)
  * [Machine learning 스터디 (13) Clustering (K-means, Gaussian Mixture Model)](http://sanghyukchun.github.io/69/)
  * [Machine Learning 스터디 (14) EM Algorithm](http://sanghyukchun.github.io/70/)
  * [Machine Learning 스터디 (16) Dimensionality Reduction (PCA, LDA)](http://sanghyukchun.github.io/72/)
* [모두를 위한 머신러닝/딥러닝 강의](http://hunkim.github.io/ml/)
  * [Lecture #31) lec10-2: Weight 초기화 잘해보자](https://www.youtube.com/watch?v=4rC0sWrp3Uw&list=PLlMkM4tgfjnLSOjrEJN31gZATbcj_MpUm&index=31)
  * Lecture #32

---

# 2019-12-21
* [숨니의 무작정 따라하기](https://sumniya.tistory.com/)
  * [[Part 4] Deep Q-Networks and Beyond](https://sumniya.tistory.com/19)
    * [Deep Q-Networks and Beyond](https://gist.github.com/denny0323/cff2684b117bc73460c1df3ab2244d40#file-4_deep-q-networks-and-beyond-ipynb)
* [SanghyukChun's Blog](http://sanghyukchun.github.io/)
  * [Machine Learning 스터디 (1) Machine Learning이란?](http://sanghyukchun.github.io/57/)
  * [Machine Learning 스터디 (2) Probability Theory](http://sanghyukchun.github.io/58/)
  * [Machine learning 스터디 (3) Overfitting](http://sanghyukchun.github.io/59/)
  * [Machine Learning 스터디 (4) Algorithm](http://sanghyukchun.github.io/60/)

---

# 2019-12-20
* [숨니의 무작정 따라하기](https://sumniya.tistory.com/)
  * [[Ch.8] Value Function Approximation](https://sumniya.tistory.com/17)
  * [[Ch.9] DQN(Deep Q-Networks)](https://sumniya.tistory.com/18)
  * [[Part 0] Q-Learning with Tables and Neural Networks](https://sumniya.tistory.com/4)
  * [[Part 1] Multi-armed Bandit](https://sumniya.tistory.com/9)
  * [[Part 1.5] Contextual Bandits](https://sumniya.tistory.com/12)
  * [[Part 2] Policy-based Agents(Cart-Pole Problem)](https://sumniya.tistory.com/13)
  * [[Part 3] Model-based RL](https://sumniya.tistory.com/16)
  * [[Part 4] Deep Q-Networks and Beyond](https://sumniya.tistory.com/19)
  * [[Part 5] Visualizing an Agent’s Thoughts and Actions](https://sumniya.tistory.com/21)
  * [[Part 6] Partial Observability and Deep Recurrent Q-Networks](https://sumniya.tistory.com/22)
    * [Deep Recurrent Q-Network](https://gist.github.com/denny0323/80c0a9692af6416756cedcb61cafb485#file-6_partial-observability-and-deep-recurrent-q-networks-ipynb)

---

# 2019-12-19
* [숨니의 무작정 따라하기](https://sumniya.tistory.com/)
  * [[Ch.7] Off-Policy Control](https://sumniya.tistory.com/15)

---

# 2019-12-17
* [숨니의 무작정 따라하기](https://sumniya.tistory.com/)
  * [[Ch.6] Temporal Difference Methods](https://sumniya.tistory.com/14)
    * [SARSA](https://gist.github.com/denny0323/b833c6c6560436338bbd469f4301c0af#file-3_sarsa-ipynb)

---

# 2019-12-16
* [숨니의 무작정 따라하기](https://sumniya.tistory.com/)
  * [[Ch.4] Dynamic Programming](https://sumniya.tistory.com/10)
    * [Policy Iteration](https://gist.github.com/denny0323/3c67330020ee7da0597edc197adfe8f0#file-1-1_policy-iteration-ipynb)
    * [Value Iteration](https://gist.github.com/denny0323/cbacc8a5417ad122b62f50e1a834274a#file-1-2_value-iteration-ipynb)
  * [[Ch.5] Monte-Calro Methods](https://sumniya.tistory.com/11)
    * [MC Control Agent](https://gist.github.com/denny0323/9ae110047d81001a11eefb57d8f76819#file-2_mc-control-agent-ipynb)

---

# 2019-12-15
* [강화학습 기초부터 DQN까지](https://www.slideshare.net/CurtPark1/dqn-reinforcement-learning-from-basics-to-dqn)
  * Page #143
* [숨니의 무작정 따라하기](https://sumniya.tistory.com/)
  * [[Ch.1] Introduction](https://sumniya.tistory.com/2)
  * [[Ch.2] Markov Decision Process](https://sumniya.tistory.com/3)
  * [[Ch.3] Bellman Equation](https://sumniya.tistory.com/5)

---

# 2019-12-14
* [Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html)
  * Chapter 5. Monte Carlo Methods
    * 5.5 Off-policy Prediction via Importance Sampling
    * 5.6 Incremental Implementation
  * Page #110
* [강화학습 기초부터 DQN까지](https://www.slideshare.net/CurtPark1/dqn-reinforcement-learning-from-basics-to-dqn)
  * Page #90

---

# 2019-12-10
* [Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html)
  * Chapter 4. Dynamic Programming
    * 4.2 Policy Improvement
    * 4.3 Policy Iteration
    * [4.4 Value Iteration](https://sumniya.tistory.com/10)
    * 4.5 Asynchronous Dynamic Programming
    * 4.6 Generalized Policy Iteration
    * 4.7 E fficiency of Dynamic Programming
    * 4.8 Summary
  * Chapter 5. Monte Carlo Methods
    * 5.1 Monte Carlo Prediction
    * 5.2 Monte Carlo Estimation of Action Values
    * 5.3 Monte Carlo Control
    * 5.4 Monte Carlo Control without Exploring Starts
    * 5.5 Off-policy Prediction via Importance Sampling
  * Page #104

---

# 2019-12-04
* [Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html)
  * Chapter 3. Finite Markov Decision Processes
    * 3.1 The Agent–Environment Interface
    * 3.2 Goals and Rewards
    * 3.3 Returns and Episodes
    * 3.4 Unified Notation for Episodic and Continuing Tasks
    * 3.5 Policies and Value Functions
    * 3.6 Optimal Policies and Optimal Value Functions
    * 3.7 Optimality and Approximation
    * 3.8 Summary
  * Chapter 4. Dynamic Programming
    * 4.1 Policy Evaluation (Prediction)
    * [4.2 Policy Improvement](https://sumniya.tistory.com/10)
  * Page #79

---

# 2019-12-03
* [Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html)
  * Chapter 2. Multi-armed Bandits
    * 2.7 Upper-Confidence-Bound Action Selection
    * 2.8 Gradient Bandit Algorithms
    * 2.9 Associative Search (Contextual Bandits)
    * 2.10 Summary
  * Page #47

---

# 2019-12-02
* [모두를 위한 머신러닝/딥러닝 강의](http://hunkim.github.io/ml/)
  * [Lecture #30) lec10-1: Sigmoid 보다 ReLU가 더 좋아](https://www.youtube.com/watch?v=cKtg_fpw88c&list=PLlMkM4tgfjnLSOjrEJN31gZATbcj_MpUm&index=31&t=0s)
* [Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html)
  * Chapter 1. Introduction
    * 1.4 Limitations and Scope
    * 1.5 An Extended Example: Tic-Tac-Toe
    * 1.6 Summary
  * Chapter 2. Multi-armed Bandits
    * 2.1 A k-armed Bandit Problem
    * 2.2 Action-value Methods
    * 2.3 The 10-armed Testbed
    * 2.4 Incremental Implementation
    * 2.5 Tracking a Nonstationary Problem
    * 2.6 Optimistic Initial Values
  * Page #35

---

# 2019-12-01
* [Fundamental of Reinforcement Learning](https://dnddnjs.gitbook.io/rl/)
  * Chapter 4. Dynamic Programming
  * Chapter 5. Monte-Carlo Methods
* [모두를 위한 머신러닝/딥러닝 강의](http://hunkim.github.io/ml/)
  * [Lecture #25) lec9-1: XOR 문제 딥러닝으로 풀기](https://www.youtube.com/watch?v=GYecDQQwTdI&list=PLlMkM4tgfjnLSOjrEJN31gZATbcj_MpUm&index=26&t=0s)
  * Lecture #26
  * Lecture #27
  * Lecture #28
  * Lecture #29
* [Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html)
  * Chapter 1. Introduction
    * 1.1 Reinforcement Learning
    * 1.2 Examples
    * 1.3 Elements of Reinforcement Learning
  * Page #7

---

# 2019-11-30
* [모두를 위한 머신러닝/딥러닝 강의](http://hunkim.github.io/ml/)
  * [Lecture #14) ML lec 6-1 - Softmax Regression: 기본 개념 소개](https://www.youtube.com/watch?v=MFAnsx1y9ZI&list=PLlMkM4tgfjnLSOjrEJN31gZATbcj_MpUm&index=14)
  * Lecture #15
  * Lecture #16
  * Lecture #17
  * Lecture #18
  * Lecture #19
  * Lecture #20
  * Lecture #21
  * Lecture #22
  * Lecture #23
  * Lecture #24
* [파이썬과 케라스로 배우는 강화학습](https://missflash.github.io/python-keras-rl/)
  * 7. 강화학습 심화 3: 아타리

---

# 2019-11-29
* [강화학습 관련 자료](https://github.com/reinforcement-learning-kr/how_to_study_rl/wiki/강화학습-관련-자료)
  * [자료 정리](https://github.com/missflash/missflash.github.io/wiki/Deep-Self-Learning)
* [Fundamental of Reinforcement Learning](https://dnddnjs.gitbook.io/rl/)
  * Chapter 1. Introduction
  * Chapter 2. Markov Decision Process
  * Chapter 3. Bellman Equation

---

# Template
* [Fundamental of Reinforcement Learning](https://dnddnjs.gitbook.io/rl/)
  * Chapter #.
* [모두를 위한 머신러닝/딥러닝 강의](http://hunkim.github.io/ml/)
  * Lecture #.
* [UCL Course on RL](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html)
  * Lecture #.
* [Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html)
  * Page #.

---
